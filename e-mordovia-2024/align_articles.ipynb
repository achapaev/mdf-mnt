{"cells":[{"cell_type":"code","execution_count":null,"id":"6c080b1c","metadata":{"id":"6c080b1c"},"outputs":[],"source":["!pip install razdel\n","!pip install xxhash"]},{"cell_type":"code","execution_count":null,"id":"3dd853c8","metadata":{"id":"3dd853c8"},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","from tqdm.auto import tqdm, trange\n","import random\n","import json\n","import razdel\n","import re\n","import matplotlib.pyplot as plt\n","import xxhash"]},{"cell_type":"code","execution_count":null,"id":"ffdad85a","metadata":{"id":"ffdad85a"},"outputs":[],"source":["import torch\n","from transformers import BertModel, BertTokenizerFast"]},{"cell_type":"code","execution_count":null,"id":"5HP9CUD6uNhv","metadata":{"id":"5HP9CUD6uNhv"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"22790625","metadata":{"id":"22790625"},"outputs":[],"source":["DATA_PATH_PREFIX = 'drive/MyDrive/diploma/data/'"]},{"cell_type":"code","execution_count":null,"id":"K2Uh4Yo_uTrB","metadata":{"id":"K2Uh4Yo_uTrB"},"outputs":[],"source":["from itertools import groupby\n","import re\n","\n","import razdel\n","\n","QUOTE_TYPE = '\"'\n","DASH_TYPE = '-'\n","\n","\n","def limit_repeated_chars(text: str, max_run: int = 3) -> str:\n","    \"\"\"\n","    Limits consecutive repeated characters to a specified maximum number.\n","\n","    Example:\n","        \"[8_________________________ 2400 3 сядт, 4 дес. 6 един.\" -> \"[8___ 2400 3 сядт, 4 дес. 6 един.\"\n","\n","    Args:\n","        text (str): The input text containing repeated characters.\n","        max_run (int, optional): The maximum number of consecutive identical characters allowed. Default is 3.\n","\n","    Returns:\n","        str: The text with excessive repeated characters trimmed.\n","    \"\"\"\n","    return ''.join(''.join(list(group)[:max_run]) for _, group in groupby(text))\n","\n","\n","def clean_text(raw_text: str) -> str:\n","    \"\"\"\n","    Cleans the input text by performing the following operations:\n","    - Replacing all quotes with the specified type.\n","    - Replacing all dashes with the specified type.\n","    - Removing hyphenation.\n","    - Limiting repeated characters.\n","    - Replacing multiple spaces with a single space.\n","    - Removing asterisks at the beginning of words.\n","    - Normalizing spacing around periods.\n","\n","    Args:\n","        raw_text (str): The input raw text.\n","\n","    Returns:\n","        str: The cleaned text.\n","    \"\"\"\n","    text = re.sub(r'[“”„‟«»‘’‚‛]', QUOTE_TYPE, raw_text)\n","#     text = re.sub(r'[‐‑‒–—―]', DASH_TYPE, text)\n","\n","    text = limit_repeated_chars(text)\n","\n","    text = re.sub('(\\. )+', '. ', text)\n","    text = text.replace('\\xa0', ' ')\n","\n","    text = re.sub('\\s+', ' ', text)\n","\n","    text = text.replace('* ', '')\n","    return text.strip()\n","\n","\n","def split_into_sentences(text: str) -> list[str]:\n","    \"\"\"\n","    Splits a given text into sentences using the Razdel library.\n","\n","    Args:\n","        text (str): The input text to be split.\n","\n","    Returns:\n","        list[str]: A list of sentences extracted from the text.\n","    \"\"\"\n","    sents = []\n","    for sent in razdel.sentenize(text):\n","        sent_text = sent.text.replace('-\\n', '').replace('\\n', ' ').strip()\n","        sents.append(sent_text)\n","    return sents"]},{"cell_type":"code","execution_count":null,"id":"k10YpdhMvA8b","metadata":{"id":"k10YpdhMvA8b"},"outputs":[],"source":["import logging\n","import typing as tp\n","\n","import numpy as np\n","import pandas as pd\n","import razdel\n","import torch\n","from tqdm.auto import trange\n","from transformers import AutoModel, AutoTokenizer\n","\n","\n","def get_top_mean_by_row(x, k=5):\n","    m, n = x.shape\n","    k = min(k, n)\n","    topk_indices = np.argpartition(x, -k, axis=1)[:, -k:]\n","    rows, _ = np.indices((m, k))\n","    return x[rows, topk_indices].mean(1)\n","\n","\n","def embed(\n","    texts, model, tokenizer, max_length=512, batch_size=16, progress=False\n",") -> np.ndarray:\n","    \"\"\"LaBSE-like sentence embeding\"\"\"\n","    if isinstance(texts, str):\n","        single = True\n","        texts = [texts]\n","    else:\n","        single = False\n","    result = []\n","    range_fn = trange if progress else range\n","    for i in range_fn(0, len(texts), batch_size):\n","        batch = texts[i : i + batch_size]\n","        encoded_input = tokenizer(\n","            batch,\n","            padding=True,\n","            truncation=True,\n","            max_length=max_length,\n","            return_tensors=\"pt\",\n","        )\n","        with torch.inference_mode():\n","            model_output = model(**encoded_input.to(model.device))\n","            result.append(\n","                torch.nn.functional.normalize(model_output.pooler_output).cpu().numpy()\n","            )\n","    embeddings = np.concatenate(result)\n","    if single:\n","        return embeddings[0]\n","    return embeddings\n","\n","\n","def align3(sims: np.ndarray) -> tp.List[tp.Tuple[int, int]]:\n","    \"\"\"\n","    Given an array of similarity values, compute a strictly monotonic path (possibly with skips)\n","    with the maximal sum of similarities along the path.\n","    Skipping happens if the similaritie are negative, so they would otherwise decrease the total.\n","    \"\"\"\n","    nrows, ncols = sims.shape\n","\n","    rewards = np.zeros_like(sims)\n","    choices = np.zeros_like(sims).astype(\n","        int\n","    )  # 1: choose this pair, 2: decrease i, 3: decrease j\n","\n","    for i in range(nrows):\n","        for j in range(ncols):\n","            # Option 1: align i to j\n","            score_add = sims[i, j]\n","            if i > 0 and j > 0:\n","                score_add += rewards[i - 1, j - 1]\n","                choices[i, j] = 1\n","            best = score_add\n","            # Option 2: skip i, align j to the best alignment before\n","            if i > 0 and rewards[i - 1, j] > best:\n","                best = rewards[i - 1, j]\n","                choices[i, j] = 2\n","            # Option 3: skip j, align i to the best alignment before\n","            if j > 0 and rewards[i, j - 1] > best:\n","                best = rewards[i, j - 1]\n","                choices[i, j] = 3\n","            rewards[i, j] = best\n","\n","    # backtracking the optimal alignment\n","    alignment = []\n","    i = nrows - 1\n","    j = ncols - 1\n","    while i >= 0 and j >= 0:\n","        if choices[i, j] in {\n","            0,\n","            1,\n","        }:  # 0 occurs only in the pair of first sentences, if we are at it\n","            alignment.append((i, j))\n","            i -= 1\n","            j -= 1\n","        elif choices[i, j] == 2:\n","            i -= 1\n","        else:\n","            j -= 1\n","    return alignment[::-1]\n","\n","\n","def get_penalized_sims(\n","    src_sents,\n","    tgt_sents,\n","    src_embs,\n","    tgt_embs,\n","    rel_penalty=0.2,\n","    abs_penalty=0.2,\n","    cosine_power=1,\n",") -> tp.Tuple[np.ndarray, np.ndarray]:\n","    len_sims = np.array(\n","        [\n","            [min(len(x), len(y)) / max(len(x), len(y)) for x in tgt_sents]\n","            for y in src_sents\n","        ]\n","    )\n","    sims = np.maximum(0, np.dot(src_embs, tgt_embs.T)) ** cosine_power * len_sims\n","    sims_rel = (\n","        (sims.T - get_top_mean_by_row(sims) * rel_penalty).T\n","        - get_top_mean_by_row(sims.T) * rel_penalty\n","        - abs_penalty\n","    )\n","    return sims, sims_rel\n","\n","\n","def align_docs(\n","    src_sents: tp.List[str],\n","    tgt_sents: tp.List[str],\n","    pair_ids: tp.List[tp.Tuple[int, int]],\n","    sims: np.ndarray,\n","    sims_rel: np.ndarray,\n",") -> pd.DataFrame:\n","    \"\"\"Align two documents into a single parallel document, possibly with gaps\"\"\"\n","    doc_sents = []\n","    prev_i, prev_j = 0, 0\n","    for pair_i, pair_j in pair_ids + [(len(src_sents), len(tgt_sents))]:\n","        for i in range(prev_i, pair_i):\n","            doc_sents.append({\"src_sent_id\": i, \"src_sent\": src_sents[i]})\n","        for j in range(prev_j, pair_j):\n","            doc_sents.append({\"tgt_sent_id\": j, \"tgt_sent\": tgt_sents[j]})\n","        if pair_i >= len(src_sents):\n","            break\n","        doc_sents.append(\n","            {\n","                \"src_sent_id\": pair_i,\n","                \"src_sent\": src_sents[pair_i],\n","                \"tgt_sent_id\": pair_j,\n","                \"tgt_sent\": tgt_sents[pair_j],\n","                \"sim\": sims[pair_i, pair_j],\n","                \"sim_pnlz\": sims_rel[pair_i, pair_j],\n","            }\n","        )\n","        prev_i, prev_j = pair_i + 1, pair_j + 1\n","\n","    doc_df = pd.DataFrame(doc_sents)\n","    return doc_df"]},{"cell_type":"code","execution_count":null,"id":"ZzMNPPypLusm","metadata":{"id":"ZzMNPPypLusm"},"outputs":[],"source":["MDF_MODEL = 'drive/MyDrive/diploma/labse_moksha_v3_500+3500_64bs_700_without_CE_teacher_2e-5_48bs_64mlm'"]},{"cell_type":"code","execution_count":null,"id":"0gdmMxDcjJu9","metadata":{"id":"0gdmMxDcjJu9"},"outputs":[],"source":["MYV_MODEL = \"slone/LaBSE-en-ru-myv-v2\""]},{"cell_type":"code","execution_count":null,"id":"dc1f6160","metadata":{"id":"dc1f6160"},"outputs":[],"source":["def resentenize_article(text):\n","    cleaned_text = clean_text(text)\n","\n","    lines = cleaned_text.split('\\n')\n","    if len(lines) == 0:\n","        return []\n","    if lines[-1] in {\n","        'Пресс-служба Главы Республики Мордовия',  # ru\n","        'Мордовия Республикань Прявтонть пресс-службась',  # myv\n","        'Мордовия Республикань Прявтонь пресс-службась',  # myv\n","        'Мордовия Республикань Оцюнять пресс-службац',  # mdf\n","    }:\n","        lines = lines[:-1]\n","\n","    return [sent for sent in split_into_sentences(' '.join(lines))]"]},{"cell_type":"markdown","id":"-tHMYagPFCk2","metadata":{"id":"-tHMYagPFCk2"},"source":["# Load  parallel texts"]},{"cell_type":"code","execution_count":null,"id":"sFYRlmGADlHY","metadata":{"id":"sFYRlmGADlHY"},"outputs":[],"source":["# lang_pair = 'MDF-RU'\n","# lang_pair = 'MYV-RU'\n","lang_pair = 'MYV-MDF'"]},{"cell_type":"code","execution_count":null,"id":"FRNQtVjjR-lo","metadata":{"id":"FRNQtVjjR-lo"},"outputs":[],"source":["split = 'train'\n","# split = 'dev'\n","# split = 'test'"]},{"cell_type":"code","execution_count":null,"id":"rdJcHRJrIEXI","metadata":{"id":"rdJcHRJrIEXI"},"outputs":[],"source":["with open(DATA_PATH_PREFIX + f\"e-mordovia/{lang_pair}_{split}.json\", 'r') as f:\n","    article2candidates = json.load(f)"]},{"cell_type":"code","execution_count":null,"id":"vnHFvcsSIEQQ","metadata":{"id":"vnHFvcsSIEQQ"},"outputs":[],"source":["len(article2candidates)"]},{"cell_type":"markdown","id":"737047ca","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"id":"PmRxzirdE5Kb","metadata":{"id":"PmRxzirdE5Kb"},"outputs":[],"source":["if 'MDF' in lang_pair:\n","    mdf_model = BertModel.from_pretrained(MDF_MODEL)\n","    mdf_tokenizer = BertTokenizerFast.from_pretrained(MDF_MODEL)\n","    mdf_model.cuda();"]},{"cell_type":"code","execution_count":null,"id":"BHZq63XkV9Z8","metadata":{"id":"BHZq63XkV9Z8"},"outputs":[],"source":["if 'MYV' in lang_pair:\n","    myv_model = BertModel.from_pretrained(MYV_MODEL)\n","    myv_tokenizer = BertTokenizerFast.from_pretrained(MYV_MODEL)\n","    myv_model.cuda();"]},{"cell_type":"code","execution_count":null,"id":"u_qbynRlW4JE","metadata":{"id":"u_qbynRlW4JE"},"outputs":[],"source":["if lang_pair == 'MDF-RU':\n","    article_lang = 'MDF'\n","    candidates_lang = 'RU'\n","\n","    src_model = tgt_model = mdf_model\n","    src_tokenizer = tgt_tokenizer = mdf_tokenizer\n","\n","elif lang_pair == 'MYV-RU':\n","    article_lang = 'MYV'\n","    candidates_lang = 'RU'\n","\n","    src_model = tgt_model = myv_model\n","    src_tokenizer = tgt_tokenizer = myv_tokenizer\n","\n","elif lang_pair == 'MYV-MDF':\n","    article_lang = 'MYV'\n","    candidates_lang = 'MDF'\n","\n","    src_model = myv_model\n","    src_tokenizer = myv_tokenizer\n","\n","    tgt_model = mdf_model\n","    tgt_tokenizer = mdf_tokenizer"]},{"cell_type":"markdown","id":"75200edd","metadata":{"id":"75200edd"},"source":["## Playing with examples"]},{"cell_type":"code","execution_count":null,"id":"0600d8ef","metadata":{"id":"0600d8ef"},"outputs":[],"source":["fn = random.choice(list(article2candidates.keys()))\n","print(fn)\n","item = article2candidates[fn]\n","print(item.keys())"]},{"cell_type":"code","execution_count":null,"id":"7c447b68","metadata":{"id":"7c447b68"},"outputs":[],"source":["src_text = item['article']['text']\n","tgt_text = item['candidates'][0]['text']"]},{"cell_type":"code","execution_count":null,"id":"4fd5617c","metadata":{"id":"4fd5617c"},"outputs":[],"source":["src_sents = resentenize_article(src_text)\n","tgt_sents = resentenize_article(tgt_text)"]},{"cell_type":"code","execution_count":null,"id":"2a2786ea","metadata":{"id":"2a2786ea"},"outputs":[],"source":["src_embs = embed(src_sents, src_model, src_tokenizer)\n","tgt_embs = embed(tgt_sents, tgt_model, tgt_tokenizer)"]},{"cell_type":"code","execution_count":null,"id":"d5f84795","metadata":{"id":"d5f84795"},"outputs":[],"source":["sims, sims_rel = get_penalized_sims(src_sents, tgt_sents, src_embs, tgt_embs, rel_penalty=0.2, abs_penalty=0.2)\n","\n","print(sims_rel.shape)\n","pair_ids = align3(sims_rel)\n","print(len(pair_ids))\n","plt.imshow(sims_rel);"]},{"cell_type":"code","execution_count":null,"id":"3035f404","metadata":{"id":"3035f404"},"outputs":[],"source":["pd.options.display.max_colwidth = 300\n","pd.options.display.max_rows = 200"]},{"cell_type":"code","execution_count":null,"id":"9ba5d35b","metadata":{"id":"9ba5d35b"},"outputs":[],"source":["doc_df = align_docs(src_sents, tgt_sents, pair_ids, sims, sims_rel)\n","doc_df['src_doc_hash'] = xxhash.xxh3_64_hexdigest(item['article']['link'])\n","doc_df['tgt_doc_hash'] = xxhash.xxh3_64_hexdigest(item['candidates'][0]['link'])\n","doc_df['docs_sim'] = doc_df.sim.fillna(0).mean()\n","\n","print('mean aligned penalized sim:', doc_df.sim_pnlz.mean())\n","print('mean gross raw sim:        ', doc_df.sim.fillna(0).mean())\n","\n","\n","doc_df"]},{"cell_type":"markdown","id":"33acdfd8","metadata":{"id":"33acdfd8"},"source":["## Running it for the whole data"]},{"cell_type":"code","execution_count":null,"id":"bf6b33ff","metadata":{"id":"bf6b33ff"},"outputs":[],"source":["print(sum(len(resentenize_article(item['article']['text'])) for item in article2candidates.values()))"]},{"cell_type":"code","execution_count":null,"id":"6c6d95e8","metadata":{"id":"6c6d95e8"},"outputs":[],"source":["aligned_docs = []\n","\n","for fn in tqdm(list(article2candidates.keys())):\n","    item = article2candidates[fn]\n","    src_text = item['article']['text']\n","    for cand in item['candidates']:\n","        tgt_text = cand['text']\n","\n","        src_sents = resentenize_article(src_text)\n","        tgt_sents = resentenize_article(tgt_text)\n","        if len(tgt_sents) == 0:  # yes, this shit sometimes happens\n","            continue\n","\n","        src_embs = embed(src_sents, src_model, src_tokenizer)\n","        tgt_embs = embed(tgt_sents, tgt_model, tgt_tokenizer)\n","\n","        sims, sims_rel = get_penalized_sims(src_sents, tgt_sents, src_embs, tgt_embs, rel_penalty=0.2, abs_penalty=0.2)\n","        pair_ids = align3(sims_rel)\n","\n","        doc_df = align_docs(src_sents, tgt_sents, pair_ids, sims, sims_rel)\n","\n","        doc_df['src_doc_link'] = item['article']['link']\n","        doc_df['tgt_doc_link'] = cand['link']\n","\n","        doc_df['src_doc_hash'] = xxhash.xxh3_64_hexdigest(item['article']['link'])\n","        doc_df['tgt_doc_hash'] = xxhash.xxh3_64_hexdigest(cand['link'])\n","\n","        doc_df['docs_sim'] = doc_df.sim.fillna(0).mean()\n","        doc_df['src_id'] = int(fn[8:-5])\n","\n","        aligned_docs.append(doc_df)"]},{"cell_type":"code","execution_count":null,"id":"c3e2983e","metadata":{"id":"c3e2983e"},"outputs":[],"source":["total_doc = pd.concat(aligned_docs, ignore_index=True)\n","print(total_doc.shape)\n","print(total_doc.dropna().shape)"]},{"cell_type":"code","execution_count":null,"id":"4292d5c0","metadata":{"id":"4292d5c0","scrolled":true},"outputs":[],"source":["total_doc.sample(10)"]},{"cell_type":"code","execution_count":null,"id":"44cbb945","metadata":{"id":"44cbb945"},"outputs":[],"source":["total_doc.sim.hist(bins=100);"]},{"cell_type":"code","execution_count":null,"id":"86fc0f28","metadata":{"id":"86fc0f28"},"outputs":[],"source":["total_doc.docs_sim.hist(bins=100);"]},{"cell_type":"code","execution_count":null,"id":"8b6e1267","metadata":{"id":"8b6e1267"},"outputs":[],"source":["total_doc.describe()"]},{"cell_type":"code","execution_count":null,"id":"Tz3db90gDgyv","metadata":{"id":"Tz3db90gDgyv"},"outputs":[],"source":["total_doc.to_parquet(DATA_PATH_PREFIX + f'e-mordovia/hf/{lang_pair}_{split}.parquet', index=False)"]},{"cell_type":"code","execution_count":null,"id":"ySM7vWFZca82","metadata":{"id":"ySM7vWFZca82"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"4GK1Pze-bVX4","metadata":{"id":"4GK1Pze-bVX4"},"source":["# iter over all pairs and all splits"]},{"cell_type":"code","execution_count":null,"id":"xaOigfpgca_Y","metadata":{"id":"xaOigfpgca_Y"},"outputs":[],"source":["for lang_pair in ['MDF-RU', 'MYV-RU', 'MYV-MDF']:\n","\n","    if lang_pair == 'MDF-RU':\n","        article_lang = 'MDF'\n","        candidates_lang = 'RU'\n","\n","        src_model = tgt_model = mdf_model\n","        src_tokenizer = tgt_tokenizer = mdf_tokenizer\n","\n","    elif lang_pair == 'MYV-RU':\n","        article_lang = 'MYV'\n","        candidates_lang = 'RU'\n","\n","        src_model = tgt_model = myv_model\n","        src_tokenizer = tgt_tokenizer = myv_tokenizer\n","\n","    elif lang_pair == 'MYV-MDF':\n","        article_lang = 'MYV'\n","        candidates_lang = 'MDF'\n","\n","        src_model = myv_model\n","        src_tokenizer = myv_tokenizer\n","\n","        tgt_model = mdf_model\n","        tgt_tokenizer = mdf_tokenizer\n","\n","    for split in ['train', 'dev', 'test']:\n","\n","        print(f\"e-mordovia/{lang_pair}_{split}.json\")\n","        with open(DATA_PATH_PREFIX + f\"e-mordovia/{lang_pair}_{split}.json\", 'r') as f:\n","            article2candidates = json.load(f)\n","\n","        aligned_docs = []\n","\n","        for fn in tqdm(list(article2candidates.keys())):\n","            item = article2candidates[fn]\n","            src_text = item['article']['text']\n","            for cand in item['candidates']:\n","                tgt_text = cand['text']\n","\n","                src_sents = resentenize_article(src_text)\n","                tgt_sents = resentenize_article(tgt_text)\n","\n","                if len(src_text) == 0 or len(tgt_sents) == 0:  # yes, this shit sometimes happens\n","                    continue\n","\n","                if lang_pair == 'MDF-RU':\n","                    src_embs = embed(src_sents, mdf_model, mdf_tokenizer)\n","                    tgt_embs = embed(tgt_sents, mdf_model, mdf_tokenizer)\n","\n","                if lang_pair == 'MYV-RU':\n","                    src_embs = embed(src_sents, myv_model, myv_tokenizer)\n","                    tgt_embs = embed(tgt_sents, myv_model, myv_tokenizer)\n","\n","                if lang_pair == 'MYV-MDF':\n","                    src_embs = embed(src_sents, myv_model, myv_tokenizer)\n","                    tgt_embs = embed(tgt_sents, mdf_model, mdf_tokenizer)\n","\n","                sims, sims_rel = get_penalized_sims(src_sents, tgt_sents, src_embs, tgt_embs, rel_penalty=0.2, abs_penalty=0.2)\n","                pair_ids = align3(sims_rel)\n","\n","                doc_df = align_docs(src_sents, tgt_sents, pair_ids, sims, sims_rel)\n","\n","                doc_df['src_doc_link'] = item['article']['link']\n","                doc_df['tgt_doc_link'] = cand['link']\n","\n","                doc_df['src_doc_hash'] = xxhash.xxh3_64_hexdigest(item['article']['link'])\n","                doc_df['tgt_doc_hash'] = xxhash.xxh3_64_hexdigest(cand['link'])\n","\n","                doc_df['docs_sim'] = doc_df.sim.fillna(0).mean()\n","                doc_df['src_id'] = int(fn[8:-5])\n","\n","                aligned_docs.append(doc_df)\n","\n","        total_doc = pd.concat(aligned_docs, ignore_index=True)\n","        print(total_doc.shape)\n","        print(total_doc.dropna().shape)\n","\n","        total_doc.to_parquet(DATA_PATH_PREFIX + f'e-mordovia/hf/{lang_pair}_{split}.parquet', index=False)\n","\n","        aligned_docs = []"]},{"cell_type":"code","execution_count":null,"id":"fGDaRS7LcJRc","metadata":{"id":"fGDaRS7LcJRc"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":5}
