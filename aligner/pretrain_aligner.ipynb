{"cells":[{"cell_type":"code","execution_count":null,"id":"b9V2MNZERFI0","metadata":{"id":"b9V2MNZERFI0"},"outputs":[],"source":["!pip install razdel\n","!pip install heapdict"]},{"cell_type":"code","execution_count":null,"id":"df2081cc","metadata":{"id":"df2081cc"},"outputs":[],"source":["from collections import Counter, defaultdict\n","import json\n","import os\n","import random\n","import itertools\n","\n","from heapdict import heapdict\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm, trange\n","import torch\n","from transformers import (\n","    Adafactor,\n","    AutoModel,\n","    AutoModelForPreTraining,\n","    AutoTokenizer,\n","    BertTokenizer,\n","    DataCollatorForWholeWordMask\n",")"]},{"cell_type":"code","execution_count":null,"id":"J4Trd6Sdk2fb","metadata":{"id":"J4Trd6Sdk2fb"},"outputs":[],"source":["import torch\n","\n","\n","def validate_model(\n","        model,\n","        tokenizer,\n","        teacher_model,\n","        teacher_tokenizer,\n","        data,\n","        batch_size,\n","):\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","\n","    losses = []\n","    for i in range(0, len(data), batch_size):\n","        current_data = data[i:i+batch_size]\n","        current_bs = len(current_data)\n","\n","        mdf = [sample[0] for sample in current_data]\n","        ru = [sample[1] for sample in current_data]\n","\n","        with torch.inference_mode():\n","            mdf_batch = tokenizer(mdf, return_tensors='pt', padding=True, truncation=True, max_length=128).to(model.device)\n","            mdf_out = model.bert(**mdf_batch, output_hidden_states=True)\n","            mdf_embeddings = torch.nn.functional.normalize(mdf_out.pooler_output)\n","\n","            ru_batch = teacher_tokenizer(ru, return_tensors='pt', padding=True, truncation=True, max_length=128).to(teacher_model.device)\n","            ru_out = teacher_model(**ru_batch, output_hidden_states=True)\n","            ru_embeddings = torch.nn.functional.normalize(ru_out.pooler_output)\n","\n","        all_scores = torch.matmul(mdf_embeddings, ru_embeddings.T)\n","\n","        loss = loss_fn(\n","            all_scores, torch.arange(current_bs, device=model.device)\n","        ) + loss_fn(\n","            all_scores.T, torch.arange(current_bs, device=model.device)\n","        )\n","\n","        losses.append(loss.item())\n","\n","    return losses"]},{"cell_type":"code","execution_count":null,"id":"_33-faRBNUFz","metadata":{"id":"_33-faRBNUFz"},"outputs":[],"source":["from itertools import groupby\n","import re\n","\n","import razdel\n","\n","QUOTE_TYPE = '\"'\n","DASH_TYPE = '-'\n","\n","\n","def remove_hyphenation(text: str) -> str:\n","    \"\"\"\n","    Removes hyphenation from a given text by merging words split with hyphens or spaces.\n","\n","    Example:\n","        \"по-\\ нимаемый иска- женный при- мер\" -> \"понимаемый искаженный пример\"\n","\n","    Args:\n","        text (str): The input text containing hyphenated words.\n","\n","    Returns:\n","        str: The text with hyphenation removed.\n","    \"\"\"\n","    return re.sub(\n","        rf'(\\w)([\\{DASH_TYPE}+]\\s+)(\\w)',\n","        lambda matchobj: matchobj.group(1) + matchobj.group(3),\n","        text\n","    )\n","\n","\n","def limit_repeated_chars(text: str, max_run: int = 3) -> str:\n","    \"\"\"\n","    Limits consecutive repeated characters to a specified maximum number.\n","\n","    Example:\n","        \"[8_________________________ 2400 3 сядт, 4 дес. 6 един.\" -> \"[8___ 2400 3 сядт, 4 дес. 6 един.\"\n","\n","    Args:\n","        text (str): The input text containing repeated characters.\n","        max_run (int, optional): The maximum number of consecutive identical characters allowed. Default is 3.\n","\n","    Returns:\n","        str: The text with excessive repeated characters trimmed.\n","    \"\"\"\n","    return ''.join(''.join(list(group)[:max_run]) for _, group in groupby(text))\n","\n","\n","def clean_text(raw_text: str) -> str:\n","    \"\"\"\n","    Cleans the input text by performing the following operations:\n","    - Replacing all quotes with the specified type.\n","    - Replacing all dashes with the specified type.\n","    - Removing hyphenation.\n","    - Limiting repeated characters.\n","    - Replacing multiple spaces with a single space.\n","    - Removing asterisks at the beginning of words.\n","    - Normalizing spacing around periods.\n","\n","    Args:\n","        raw_text (str): The input raw text.\n","\n","    Returns:\n","        str: The cleaned text.\n","    \"\"\"\n","    text = re.sub(r'[“”„‟«»‘’‚‛]', QUOTE_TYPE, raw_text)\n","#     text = re.sub(r'[‐‑‒–—―]', DASH_TYPE, text)\n","\n","    text = remove_hyphenation(text)\n","    text = limit_repeated_chars(text)\n","\n","    text = re.sub('(\\. )+', '. ', text)\n","    text = text.replace('\\xa0', ' ')\n","\n","    text = re.sub('\\s+', ' ', text)\n","\n","    text = text.replace('* ', '')\n","    return text.strip()\n","\n","\n","def split_into_sentences(text: str) -> list[str]:\n","    \"\"\"\n","    Splits a given text into sentences using the Razdel library.\n","\n","    Args:\n","        text (str): The input text to be split.\n","\n","    Returns:\n","        list[str]: A list of sentences extracted from the text.\n","    \"\"\"\n","    sents = []\n","    for sent in razdel.sentenize(text):\n","        sent_text = sent.text.replace('-\\n', '').replace('\\n', ' ').strip()\n","        sents.append(sent_text)\n","    return sents\n","\n","\n","def is_text_valid(text: str) -> bool:\n","    \"\"\"\n","    Checks if the given text meets validity criteria:\n","    - Contains at least one word with two or more characters.\n","    - Contains at least one Cyrillic letter.\n","    - Has a length between 3 and 500 characters.\n","\n","    Args:\n","        text (str): The input text to validate.\n","\n","    Returns:\n","        bool: True if the text is valid, False otherwise.\n","    \"\"\"\n","    if max(len(w) for w in text.split()) < 2:\n","        return False\n","\n","    if not re.match('.*[а-яё].*', text.lower()):\n","        return False\n","\n","    if len(text) < 3:\n","        return False\n","\n","    if len(text) > 500:\n","        return False\n","\n","    return True\n"]},{"cell_type":"code","execution_count":null,"id":"SxxW6WJtPyLu","metadata":{"id":"SxxW6WJtPyLu"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"boTvpSwLRdgc","metadata":{"id":"boTvpSwLRdgc"},"outputs":[],"source":["DATA_PATH_PREFIX = 'drive/MyDrive/diploma/data/'"]},{"cell_type":"code","execution_count":null,"id":"OWHr2nU8trBu","metadata":{"id":"OWHr2nU8trBu"},"outputs":[],"source":["BASE_MODEL = 'cointegrated/LaBSE-en-ru'"]},{"cell_type":"code","execution_count":null,"id":"kbUYLeoZPZCn","metadata":{"id":"kbUYLeoZPZCn"},"outputs":[],"source":["SEED=13"]},{"cell_type":"markdown","id":"83617225","metadata":{"id":"83617225"},"source":["# Collect the data"]},{"cell_type":"markdown","id":"f2c19323","metadata":{"id":"f2c19323"},"source":["## Monolang data"]},{"cell_type":"markdown","id":"jM5UfmAKsHu9","metadata":{"id":"jM5UfmAKsHu9"},"source":["1. Monolingual books\n","2. Moksha pravda"]},{"cell_type":"code","execution_count":null,"id":"4qngbdj9rJYg","metadata":{"id":"4qngbdj9rJYg"},"outputs":[],"source":["books_sents = []"]},{"cell_type":"code","execution_count":null,"id":"d586c267","metadata":{"id":"d586c267","scrolled":true},"outputs":[],"source":["book_dir = DATA_PATH_PREFIX + 'mdf_mono/'\n","\n","for fn in os.listdir(book_dir):\n","    if not fn.endswith('.txt'):\n","      continue\n","\n","    print(fn)\n","    with open(book_dir + fn, 'r') as f:\n","        raw_lines = f.readlines()\n","\n","    raw_text = ''.join(raw_lines)\n","    text = clean_text(raw_text)\n","\n","    sents = []\n","    for sent in split_into_sentences(text):\n","        if not is_text_valid(sent):\n","            continue\n","        sents.append(sent)\n","    print(len(sents))\n","\n","    books_sents.extend(sents)\n","\n","print()\n","print(len(books_sents))"]},{"cell_type":"code","execution_count":null,"id":"OGzazWxKMsxH","metadata":{"id":"OGzazWxKMsxH"},"outputs":[],"source":["df_moksha_pravda = pd.read_csv(DATA_PATH_PREFIX + 'moksha_pravda.tsv', sep='\\t')"]},{"cell_type":"code","execution_count":null,"id":"sg-W_cSIM77q","metadata":{"id":"sg-W_cSIM77q"},"outputs":[],"source":["moksha_pravda_sents = []"]},{"cell_type":"code","execution_count":null,"id":"QMshvFksM9Br","metadata":{"id":"QMshvFksM9Br"},"outputs":[],"source":["for raw_text in df_moksha_pravda['title']:\n","    text = clean_text(raw_text)\n","\n","    sents = []\n","    splits = split_into_sentences(text)\n","\n","    for sent in splits:\n","        if not is_text_valid(sent):\n","            continue\n","        sents.append(sent)\n","\n","    moksha_pravda_sents.extend(sents)\n","\n","len(moksha_pravda_sents)"]},{"cell_type":"code","execution_count":null,"id":"l0HJHhoZM9GE","metadata":{"id":"l0HJHhoZM9GE"},"outputs":[],"source":["for raw_text in df_moksha_pravda['body']:\n","    text = clean_text(raw_text)\n","\n","    sents = []\n","    splits = split_into_sentences(text)\n","\n","    for sent in splits:\n","        if not is_text_valid(sent):\n","            continue\n","        sents.append(sent)\n","\n","    moksha_pravda_sents.extend(sents)\n","\n","len(moksha_pravda_sents)"]},{"cell_type":"code","execution_count":null,"id":"98d0d77e","metadata":{"id":"98d0d77e"},"outputs":[],"source":["mdf_sentences = sorted(set(\n","    books_sents + moksha_pravda_sents\n","))\n","\n","print(len(mdf_sentences))"]},{"cell_type":"markdown","id":"1f9540b9","metadata":{"id":"1f9540b9"},"source":["## Sentence-parallel data"]},{"cell_type":"markdown","id":"5K3eUgIJsnOu","metadata":{"id":"5K3eUgIJsnOu"},"source":["1. Parsed dictionaries (3.6k pairs of words and 700 pairs of phrases)\n","2. The Bible - 12k pairs\n","3. e-mordovia news - 66k pairs\n","4. dump of wikisource - 20k pairs\n","5. dump of wikipedia - 1400 low-quality pairs\n","\n","\n","Also add long sentences from parallel data into `mdf_sentences`"]},{"cell_type":"code","execution_count":null,"id":"6fbb0701","metadata":{"id":"6fbb0701"},"outputs":[],"source":["with open(DATA_PATH_PREFIX + 'train_test_splitting/train.json', 'r') as f:\n","    parallel_pairs = json.load(f)\n","print(len(parallel_pairs))\n","\n","parallel_pairs = sorted({\n","    tuple(pair) for pair in parallel_pairs\n","    if pair[0] and pair[1]\n","})\n","print(len(parallel_pairs))"]},{"cell_type":"code","execution_count":null,"id":"xuOhRZcwrpMi","metadata":{"id":"xuOhRZcwrpMi"},"outputs":[],"source":["random.sample(parallel_pairs, 10)"]},{"cell_type":"code","execution_count":null,"id":"5d0c27f5","metadata":{"id":"5d0c27f5"},"outputs":[],"source":["mdf_sentences = sorted(set(\n","    mdf_sentences + [mdf for mdf, ru in parallel_pairs if len(mdf.split()) >= 3]\n","))\n","print(len(mdf_sentences))"]},{"cell_type":"markdown","id":"Pel2raYTUPYL","metadata":{"id":"Pel2raYTUPYL"},"source":["load only words"]},{"cell_type":"code","execution_count":null,"id":"DocPuki4UBDw","metadata":{"id":"DocPuki4UBDw"},"outputs":[],"source":["word_df = pd.read_csv(DATA_PATH_PREFIX + 'all_dicts_data.tsv', sep='\\t')\n","\n","assert not word_df.isna().sum().sum()\n","\n","word_pairs = sorted(list(zip(word_df['mdf'], word_df['ru'])))\n","\n","print(len(word_pairs))\n","print(random.choice(word_pairs))"]},{"cell_type":"markdown","id":"KFydRd9gi85r","metadata":{"id":"KFydRd9gi85r"},"source":["# Load dev set"]},{"cell_type":"code","execution_count":null,"id":"wV_lG-dKjEXq","metadata":{"id":"wV_lG-dKjEXq"},"outputs":[],"source":["with open(DATA_PATH_PREFIX + 'train_test_splitting/dev.json', 'r') as f:\n","    dev_pairs = json.load(f)\n","print(len(dev_pairs))\n","\n","print([(k, len(v)) for k, v in dev_pairs.items()])"]},{"cell_type":"markdown","id":"2e998e3b","metadata":{"id":"2e998e3b"},"source":["# Model vocabulary analisis and update"]},{"cell_type":"code","execution_count":null,"id":"ae7b3576","metadata":{"id":"ae7b3576"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)"]},{"cell_type":"code","execution_count":null,"id":"8aee2796","metadata":{"id":"8aee2796"},"outputs":[],"source":["print(tokenizer.vocab_size)"]},{"cell_type":"markdown","id":"1237285b","metadata":{"id":"1237285b"},"source":["## get stat for each word in corpora"]},{"cell_type":"code","execution_count":null,"id":"a9b84b5e","metadata":{"id":"a9b84b5e"},"outputs":[],"source":["word_count = Counter()\n","\n","for text in tqdm(mdf_sentences):\n","    word_count.update(t[0] for t in tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text))"]},{"cell_type":"code","execution_count":null,"id":"46f93c4b","metadata":{"id":"46f93c4b"},"outputs":[],"source":["print(len(word_count))\n","word_count.most_common(20)"]},{"cell_type":"markdown","id":"73ad74e0","metadata":{"id":"73ad74e0"},"source":["## get most frequent tokens pairs in corpora"]},{"cell_type":"code","execution_count":null,"id":"2a268df2","metadata":{"id":"2a268df2"},"outputs":[],"source":["pairs_count = Counter()\n","pair2word = defaultdict(set)\n","\n","for w, c in tqdm(word_count.items(), total=len(word_count)):\n","    tokens = tokenizer.tokenize(w)\n","    for pair in zip(tokens[:-1], tokens[1:]):\n","        pairs_count[pair] += c\n","        pair2word[pair].add(w)"]},{"cell_type":"code","execution_count":null,"id":"9b197d17","metadata":{"id":"9b197d17"},"outputs":[],"source":["# Create a heap dictionary to efficiently retrieve\n","# the most frequent token pair at each step\n","\n","hd = heapdict()\n","\n","for w, c in pairs_count.items():\n","    hd[w] = -c"]},{"cell_type":"markdown","id":"02449383","metadata":{"id":"02449383"},"source":["## replace frequent pair by their concat"]},{"cell_type":"code","execution_count":null,"id":"d47c99be","metadata":{"id":"d47c99be"},"outputs":[],"source":["replace_count = 100_000\n","min_frequency = 30"]},{"cell_type":"code","execution_count":null,"id":"0f57c3b4","metadata":{"id":"0f57c3b4"},"outputs":[],"source":["# List where each element contains a list of base token IDs\n","# Used to compute the initial weight values for new tokens\n","id2ids = [[idx] for tok, idx in tokenizer.vocab.items()]\n","\n","# Dictionary for quickly retrieving a token's index\n","# For new tokens maps new token index and index of base tokens\n","tok2id = {tok: idx for tok, idx in tokenizer.vocab.items()}\n","\n","# Dictionary to get the updated representation of words in the vocabulary\n","# Maps each word to its tokenized form using the WordPiece tokenizer\n","word2toks = {w: tokenizer.tokenize(w) for w in tqdm(word_count)}\n"]},{"cell_type":"code","execution_count":null,"id":"6de87d25","metadata":{"id":"6de87d25"},"outputs":[],"source":["def get_new_tokens_list(old_tokens, pair, new_token):\n","    result = []\n","\n","    prev = old_tokens[0]\n","    for tok in old_tokens[1:]:\n","        if (prev, tok) == pair:\n","            result.append(new_token)\n","            prev = None\n","        else:\n","            if prev is not None:\n","                result.append(prev)\n","            prev = tok\n","    if prev is not None:\n","        result.append(prev)\n","\n","    return result"]},{"cell_type":"code","execution_count":null,"id":"d325f095","metadata":{"id":"d325f095"},"outputs":[],"source":["extra_vocab = []\n","extra_counts = []"]},{"cell_type":"code","execution_count":null,"id":"844b6927","metadata":{"id":"844b6927"},"outputs":[],"source":["# Retrieve the most frequent token pair\n","# Replace it with their concatenation\n","# Update statistics for each word using the new token\n","# Update statistics for all token pairs\n","\n","for _ in trange(replace_count):\n","    pair, count = hd.peekitem()\n","    count = -count  # Convert back to positive count\n","\n","    if count < min_frequency:\n","        break\n","\n","    # Create a new token by concatenating the pair\n","    # Use [2:] to remove the '##' prefix from the second token\n","    new_token = pair[0] + pair[1][2:]\n","    extra_vocab.append(new_token)\n","    extra_counts.append(count)\n","\n","    # Update the vocabulary with the new token\n","    tok2id[new_token] = len(id2ids)\n","    id2ids.append(id2ids[tok2id[pair[0]]] + id2ids[tok2id[pair[1]]])\n","\n","    # Compute frequency changes for the heap\n","    delta = Counter()\n","    for word in list(pair2word[pair]):\n","        # Get the old and new tokenized versions of the word\n","        old_toks = word2toks[word]\n","        new_toks = get_new_tokens_list(old_toks, pair, new_token)\n","\n","        word2toks[word] = new_toks\n","        wc = word_count[word]\n","\n","        # Subtract frequency for old token pairs\n","        # Remove word associations for the replaced pairs and unchanged pairs\n","        for old_pair in zip(old_toks[:-1], old_toks[1:]):\n","            delta[old_pair] -= wc\n","            if word in pair2word[old_pair]:\n","                pair2word[old_pair].remove(word)\n","\n","        # Add frequency for new token pairs\n","        # Update word associations for the new and unchanged pairs\n","        for new_pair in zip(new_toks[:-1], new_toks[1:]):\n","            delta[new_pair] += wc\n","            pair2word[new_pair].add(word)\n","\n","    # Update the heap with new frequency values\n","    for a_pair, a_delta in delta.items():\n","        if a_delta == 0:\n","            continue\n","        if a_pair not in hd:\n","            hd[a_pair] = 0\n","        hd[a_pair] -= a_delta\n"]},{"cell_type":"markdown","id":"77c82440","metadata":{"id":"77c82440"},"source":["## update tokenizer"]},{"cell_type":"code","execution_count":null,"id":"dcb1b3e5","metadata":{"id":"dcb1b3e5"},"outputs":[],"source":["print(len(extra_vocab))"]},{"cell_type":"code","execution_count":null,"id":"0eb46416","metadata":{"id":"0eb46416"},"outputs":[],"source":["tmp_tok = 'tmp_tok'\n","tokenizer.save_pretrained(tmp_tok)"]},{"cell_type":"code","execution_count":null,"id":"de7e56f9","metadata":{"id":"de7e56f9"},"outputs":[],"source":["with open(tmp_tok + '/vocab.txt', 'a') as f:\n","    for token in extra_vocab:\n","        f.write(token + '\\n')"]},{"cell_type":"code","execution_count":null,"id":"xvHiyeSdbrK2","metadata":{"id":"xvHiyeSdbrK2"},"outputs":[],"source":["new_tokenizer = BertTokenizer.from_pretrained(tmp_tok)"]},{"cell_type":"code","execution_count":null,"id":"b87913b9","metadata":{"id":"b87913b9"},"outputs":[],"source":["len(tokenizer.vocab) + len(tokenizer.get_added_vocab())"]},{"cell_type":"code","execution_count":null,"id":"985b1b6d","metadata":{"id":"985b1b6d"},"outputs":[],"source":["len(new_tokenizer.vocab) + len(new_tokenizer.get_added_vocab())"]},{"cell_type":"code","execution_count":null,"id":"05261d7b","metadata":{"id":"05261d7b"},"outputs":[],"source":["random.seed(1)\n","sample_texts = random.choices(mdf_sentences, k=1000)"]},{"cell_type":"code","execution_count":null,"id":"1c0c7165","metadata":{"id":"1c0c7165"},"outputs":[],"source":["old_len = np.mean([len(tokenizer.tokenize(t)) for t in sample_texts])\n","print(old_len)"]},{"cell_type":"code","execution_count":null,"id":"aeda32a7","metadata":{"id":"aeda32a7"},"outputs":[],"source":["new_len = np.mean([len(new_tokenizer.tokenize(t)) for t in sample_texts])\n","print(new_len)"]},{"cell_type":"code","execution_count":null,"id":"d1d0fdb2","metadata":{"id":"d1d0fdb2"},"outputs":[],"source":["print(new_len / old_len)"]},{"cell_type":"markdown","id":"b6b27915","metadata":{"id":"b6b27915"},"source":["## save model for new vocab"]},{"cell_type":"code","execution_count":null,"id":"ac95270d","metadata":{"id":"ac95270d"},"outputs":[],"source":["model = AutoModelForPreTraining.from_pretrained(BASE_MODEL)"]},{"cell_type":"code","execution_count":null,"id":"0380e18f","metadata":{"id":"0380e18f"},"outputs":[],"source":["model.resize_token_embeddings(new_tokenizer.vocab_size)"]},{"cell_type":"code","execution_count":null,"id":"bcf82698","metadata":{"id":"bcf82698"},"outputs":[],"source":["for i, ids_from in enumerate(tqdm(id2ids)):\n","    if len(ids_from) == 1:\n","        continue\n","    model.bert.embeddings.word_embeddings.weight.data[i] = model.bert.embeddings.word_embeddings.weight.data[ids_from].mean(0)"]},{"cell_type":"code","execution_count":null,"id":"b73d8c81","metadata":{"id":"b73d8c81"},"outputs":[],"source":["NEW_MODEL_NAME = 'drive/MyDrive/diploma/labse_moksha_v0'\n","model.save_pretrained(NEW_MODEL_NAME)\n","new_tokenizer.save_pretrained(NEW_MODEL_NAME)"]},{"cell_type":"markdown","id":"c8447cc7","metadata":{"id":"c8447cc7"},"source":["# Training the model: base"]},{"cell_type":"code","execution_count":null,"id":"41253042","metadata":{"id":"41253042"},"outputs":[],"source":["def get_acc(e1, e2):\n","    batch_size = e1.shape[0]\n","    with torch.no_grad():\n","        scores = torch.matmul(e1, e2.T).cpu().numpy()\n","    a1 = (scores.argmax(1) == np.arange(batch_size)).mean()\n","    a2 = (scores.argmax(0) == np.arange(batch_size)).mean()\n","    return (a1 + a2) / 2"]},{"cell_type":"code","execution_count":null,"id":"d8fd21bb","metadata":{"id":"d8fd21bb"},"outputs":[],"source":["def test_model(model, tokenizer, teacher_model, teacher_tokenizer):\n","    with torch.inference_mode():\n","        test_ru = [\n","            'картофель',\n","            'резать хлеб',\n","            '- Поэтому, прежде всего, я бы хотел поздравить вас с профессиональным праздником.',\n","            'Возле костра стоял большой, перепачканный сажей жестяной чайник.',\n","            '— Сидишь, положим, на возу, а ребята сдалька завидят: \"Чапаев идет, Чапаев идет...\"',\n","        ]\n","        test_mdf = [\n","            'модамарь',\n","            'керемс кши',\n","            '– Сяс, васендакиге, монь мялезе поздравляндамс тинь профессиональнай илантень мархта.',\n","            'Толнять тейса ащесь оцю соду жестень чайник.',\n","            '— Ащат озада, мярьктяма, усф лангса, а цёратне ичкозде няйсазь: \"Чапаевсь сай, Чапаевсь сай...\"',\n","        ]\n","\n","        mdf_batch = tokenizer(test_mdf, return_tensors='pt', padding=True, truncation=True, max_length=128).to(model.device)\n","        mdf_out = model.bert(**mdf_batch, output_hidden_states=True)\n","        mdf_embeddings = torch.nn.functional.normalize(mdf_out.pooler_output)\n","\n","        ru_batch = teacher_tokenizer(test_ru, return_tensors='pt', padding=True, truncation=True, max_length=128).to(teacher_model.device)\n","        ru_out = teacher_model(**ru_batch, output_hidden_states=True)\n","        ru_embeddings = torch.nn.functional.normalize(ru_out.pooler_output)\n","\n","    alignment = torch.matmul(\n","        mdf_embeddings,\n","        ru_embeddings.T\n","    )\n","\n","    return alignment"]},{"cell_type":"code","execution_count":null,"id":"76868527","metadata":{"id":"76868527"},"outputs":[],"source":["teacher_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n","teacher_model = AutoModel.from_pretrained(BASE_MODEL)"]},{"cell_type":"code","execution_count":null,"id":"2i1g4BNAxV08","metadata":{"id":"2i1g4BNAxV08"},"outputs":[],"source":["teacher_model.cuda();"]},{"cell_type":"code","execution_count":null,"id":"zsjDd8BWPNkv","metadata":{"id":"zsjDd8BWPNkv"},"outputs":[],"source":["random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"]},{"cell_type":"code","execution_count":null,"id":"59f2f174","metadata":{"id":"59f2f174"},"outputs":[],"source":["MODEL_DIR = 'drive/MyDrive/diploma/labse_moksha_v0'\n","# MODEL_DIR = 'drive/MyDrive/diploma/labse_moksha_v3_500__64bs'\n","# MODEL_DIR = 'drive/MyDrive/diploma/labse_moksha_v3_500+3500_64bs'\n","\n","model = AutoModelForPreTraining.from_pretrained(MODEL_DIR)\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)"]},{"cell_type":"code","execution_count":null,"id":"wcUl3jDGqAE8","metadata":{"id":"wcUl3jDGqAE8"},"outputs":[],"source":["model.cuda();"]},{"cell_type":"markdown","id":"6f0c1098","metadata":{"id":"6f0c1098"},"source":["# Training the model: 1 - training of embeddings"]},{"cell_type":"code","execution_count":null,"id":"f793e88b","metadata":{"id":"f793e88b"},"outputs":[],"source":["for p in model.parameters():\n","    p.requires_grad = False\n","for p in model.bert.embeddings.word_embeddings.parameters():\n","    p.requires_grad = True"]},{"cell_type":"code","execution_count":null,"id":"de7cfd85","metadata":{"id":"de7cfd85"},"outputs":[],"source":["BATCH_SIZE = 64\n","MARGIN = 0.3\n","LR = 5e-4\n","CLIP_THRESHOLD = 1.0"]},{"cell_type":"code","execution_count":null,"id":"bef77b20","metadata":{"id":"bef77b20"},"outputs":[],"source":["optimizer = Adafactor(\n","    [p for p in model.parameters() if p.requires_grad],\n","    scale_parameter=False,\n","    relative_step=False,\n","    lr=LR,\n","    clip_threshold=CLIP_THRESHOLD\n",")"]},{"cell_type":"code","execution_count":null,"id":"a4c91150","metadata":{"collapsed":true,"id":"a4c91150","scrolled":true},"outputs":[],"source":["def train_alignment(parallel_pairs, step_count, optimizer, f=None, batch_size=BATCH_SIZE):\n","    losses = []\n","    accuracies = []\n","\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","\n","    model.train()\n","    for i in trange(step_count):\n","        mdf, ru = [list(p) for p in zip(*random.choices(parallel_pairs, k=batch_size))]\n","        try:\n","            tm, tt = (teacher_model, teacher_tokenizer)\n","            # tm, tt = (model.bert, tokenizer)\n","            # tm, tt = (teacher_model, teacher_tokenizer) if random.random() < 0.5 else (model.bert, tokenizer)\n","\n","            ru_batch = tt(ru, return_tensors='pt', padding=True, truncation=True, max_length=128)\n","            with torch.no_grad():\n","                ru_emb = torch.nn.functional.normalize(tm(**ru_batch.to(teacher_model.device)).pooler_output)\n","\n","            mdf_batch = tokenizer(mdf, return_tensors='pt', padding=True, truncation=True, max_length=128)\n","            mdf_emb = torch.nn.functional.normalize(model.bert(**mdf_batch.to(model.device)).pooler_output)\n","\n","            all_scores = torch.matmul(ru_emb, mdf_emb.T) - torch.eye(batch_size, device=model.device) * MARGIN\n","\n","            loss = loss_fn(\n","                all_scores, torch.arange(batch_size, device=model.device)\n","            ) + loss_fn(\n","                all_scores.T, torch.arange(batch_size, device=model.device)\n","            )\n","            loss.backward()\n","\n","            losses.append(loss.item())\n","            accuracies.append(get_acc(ru_emb, mdf_emb))\n","\n","            optimizer.step()\n","            optimizer.zero_grad(set_to_none=True)\n","        except RuntimeError:\n","            optimizer.zero_grad(set_to_none=True)\n","            batch, embeddings, all_scores, loss = None, None, None, None\n","            print('error', max(len(s) for s in mdf + ru))\n","            continue\n","        if (i + 1) % 20 == 0:\n","            print(i + 1, np.mean(losses[-20:]), np.mean(accuracies[-20:]))\n","            if f is not None:\n","                f.write(f\"{i + 1} {np.mean(losses[-20:])} {np.mean(accuracies[-20:])}\\n\")\n","\n","    return losses, accuracies"]},{"cell_type":"code","source":["MODEL_ID = \"drive/MyDrive/diploma/labse_moksha_v3_{}__\" + f\"{BATCH_SIZE}bs\"\n","\n","for i in range(1, 3):\n","    print(i)\n","    with open(f\"{MODEL_ID}.txt\", \"a\") as f:\n","        losses, accuracies = train_alignment(parallel_pairs, 250, optimizer, f)\n","\n","    print(test_model(model, tokenizer, teacher_model, teacher_tokenizer).cpu())\n","\n","    all_losses = list(itertools.chain(*[validate_model(model, tokenizer, teacher_model, teacher_tokenizer, pairs, 10) for pairs in dev_pairs.values()]))\n","    print(sum(all_losses) / len(all_losses))\n","\n","    with open(f\"{MODEL_ID}.txt\", \"a\") as f:\n","        f.write(f\"\\n{sum(all_losses) / len(all_losses)}\\n\")\n","\n","\n","    NEW_MODEL_NAME =MODEL_ID.format(i*250)\n","    model.save_pretrained(NEW_MODEL_NAME)\n","    tokenizer.save_pretrained(NEW_MODEL_NAME)\n","    print()\n","    print()"],"metadata":{"id":"M2Or_eYKtkY1"},"id":"M2Or_eYKtkY1","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"mV4NNT2cvQvU","metadata":{"collapsed":true,"id":"mV4NNT2cvQvU"},"outputs":[],"source":["MODEL_ID = \"drive/MyDrive/diploma/labse_moksha_v3_500+{}_\" + f\"{BATCH_SIZE}bs\"\n","\n","for i in range(1, 8):\n","    print(i)\n","    with open(f\"{MODEL_ID}.txt\", \"a\") as f:\n","        losses, accuracies = train_alignment(parallel_pairs, 500, optimizer, f)\n","\n","    print(test_model(model, tokenizer, teacher_model, teacher_tokenizer).cpu())\n","\n","    all_losses = list(itertools.chain(*[validate_model(model, tokenizer, teacher_model, teacher_tokenizer, pairs, 10) for pairs in dev_pairs.values()]))\n","    print(sum(all_losses) / len(all_losses))\n","\n","    with open(f\"{MODEL_ID}.txt\", \"a\") as f:\n","        f.write(f\"\\n{sum(all_losses) / len(all_losses)}\\n\")\n","\n","\n","    NEW_MODEL_NAME =MODEL_ID.format(i*500)\n","    model.save_pretrained(NEW_MODEL_NAME)\n","    tokenizer.save_pretrained(NEW_MODEL_NAME)\n","    print()\n","    print()"]},{"cell_type":"code","execution_count":null,"id":"dObGzvpzRo3J","metadata":{"id":"dObGzvpzRo3J"},"outputs":[],"source":["print(test_model(model, tokenizer, teacher_model, teacher_tokenizer).cpu())\n","\n","all_losses = list(itertools.chain(*[validate_model(model, tokenizer, teacher_model, teacher_tokenizer, pairs, 10) for pairs in dev_pairs.values()]))\n","print(sum(all_losses) / len(all_losses))"]},{"cell_type":"markdown","id":"c5b8dc8f","metadata":{"id":"c5b8dc8f"},"source":["# Training the model: 2 - full model training with MLM, CE"]},{"cell_type":"markdown","id":"047f040d","metadata":{"id":"047f040d"},"source":["Two modifications to the model:\n","* train to make embeddings close to that of the original LaBSE model (to avoid drifting both ru and mdf embeddings away)\n","* train on non-parallel sentences with MLM loss"]},{"cell_type":"code","execution_count":null,"id":"40f26d58","metadata":{"id":"40f26d58"},"outputs":[],"source":["for p in model.parameters():\n","    p.requires_grad = True"]},{"cell_type":"code","execution_count":null,"id":"735a963d","metadata":{"id":"735a963d"},"outputs":[],"source":["BATCH_SIZE = 48\n","MLM_BATCH_SIZE = 64\n","CE_BATCH_SIZE = 24\n","LR = 2e-5\n","MARGIN = 0.3\n","CLIP_THRESHOLD = 1.0"]},{"cell_type":"markdown","id":"404cfb0f","metadata":{"id":"404cfb0f"},"source":["## setup CE"]},{"cell_type":"code","execution_count":null,"id":"5d3ddc44","metadata":{"id":"5d3ddc44"},"outputs":[],"source":["def corrupt_pair(pair, p_edit=0.5):\n","    \"\"\" Corrupt one (randomly chosen) sentence in a pair \"\"\"\n","    pair = list(pair)\n","    ix = random.choice([0, 1])\n","\n","    sent = pair[ix].split()\n","    old_sent = sent[:]\n","    while sent == old_sent:\n","        # insert a random word\n","        if random.random() < p_edit or len(sent) == 1:\n","            other_sent = random.choice(parallel_pairs)[ix].split()\n","            sent.insert(random.randint(0, len(sent) - 1), random.choice(other_sent))\n","\n","        # replace a random word\n","        if random.random() < p_edit and len(sent) > 1:\n","            other_sent = random.choice(parallel_pairs)[ix].split()\n","            sent[random.randint(0, len(sent) - 1)] = random.choice(other_sent)\n","\n","        # remove a word\n","        if random.random() < p_edit and len(sent) > 1:\n","            sent.pop(random.randint(0, len(sent) - 1))\n","\n","        # swap words\n","        if random.random() < p_edit and len(sent) > 1:\n","            i, j = random.sample(range(len(sent)), 2)\n","            sent[i], sent[j] = sent[j], sent[i]\n","\n","    pair[ix] = ' '.join(sent)\n","    return pair"]},{"cell_type":"code","execution_count":null,"id":"b08d2517","metadata":{"id":"b08d2517"},"outputs":[],"source":["short_pairs = [p for p in tqdm(parallel_pairs) if len(tokenizer.encode(*p)) <= 100]\n","print(len(parallel_pairs), len(short_pairs))"]},{"cell_type":"code","execution_count":null,"id":"bb2d60e6","metadata":{"id":"bb2d60e6"},"outputs":[],"source":["def get_pairs_batch(batch_size=4):\n","    pairs = random.choices(short_pairs, k=int(np.ceil(batch_size / 2)))\n","\n","    labels = [1] * len(pairs) + [0] * len(pairs)\n","    if random.random() < 0.5:\n","        # make negatives by swapping sentence with a random one\n","        pairs.extend([(pairs[i][0], pairs[i-1][1]) for i in range(len(pairs))])\n","    else:\n","        # make negatives by corrupting existing sentences\n","        pairs.extend([corrupt_pair(pair) for pair in pairs])\n","\n","    pairs = [[x, y] if random.random() < 0.5 else [y, x] for x, y in pairs]\n","\n","    return [list(t) for t in zip(*pairs)], labels"]},{"cell_type":"markdown","id":"d1037df8","metadata":{"id":"d1037df8"},"source":["## setup other training parts"]},{"cell_type":"code","execution_count":null,"id":"f7b1808f","metadata":{"id":"f7b1808f"},"outputs":[],"source":["collator = DataCollatorForWholeWordMask(tokenizer, mlm=True, mlm_probability=0.3)"]},{"cell_type":"code","execution_count":null,"id":"a0bc45dc","metadata":{"id":"a0bc45dc"},"outputs":[],"source":["optimizer = Adafactor(\n","    [p for p in model.parameters() if p.requires_grad],\n","    scale_parameter=False,\n","    relative_step=False,\n","    lr=LR,\n","    clip_threshold=CLIP_THRESHOLD\n",")"]},{"cell_type":"code","execution_count":null,"id":"fd911191","metadata":{"id":"fd911191"},"outputs":[],"source":["def train_alignment_with_MLM_CE(\n","    parallel_pairs,\n","    mdf_sentences,\n","    step_count,\n","    optimizer,\n","    f=None,\n","    batch_size=BATCH_SIZE,\n","    mlm_batch_size=MLM_BATCH_SIZE\n","):\n","    losses = []\n","    accuracies = []\n","    losses_mlm = []\n","    losses_ce = []\n","\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","\n","    model.train()\n","    for i in trange(step_count):\n","        mdf, ru = [list(p) for p in zip(*random.choices(parallel_pairs, k=batch_size))]\n","        try:\n","            # translation ranking step step\n","            # in half cases, pull embeddings to the teacher; in other half - to self.\n","            tm, tt = (teacher_model, teacher_tokenizer) # if random.random() < 0.5 else (model.bert, tokenizer)\n","\n","            ru_batch = tt(ru, return_tensors='pt', padding=True, truncation=True, max_length=128)\n","            with torch.no_grad():\n","                ru_emb = torch.nn.functional.normalize(tm(**ru_batch.to(teacher_model.device)).pooler_output)\n","\n","            mdf_batch = tokenizer(mdf, return_tensors='pt', padding=True, truncation=True, max_length=128)\n","            mdf_emb = torch.nn.functional.normalize(model.bert(**mdf_batch.to(model.device)).pooler_output)\n","            all_scores = torch.matmul(ru_emb, mdf_emb.T) - torch.eye(batch_size, device=model.device) * MARGIN\n","\n","            loss = loss_fn(all_scores, torch.arange(batch_size, device=model.device)) + loss_fn(all_scores.T, torch.arange(batch_size, device=model.device))\n","            loss.backward()\n","\n","            losses.append(loss.item())\n","            accuracies.append(get_acc(mdf_emb, ru_emb))\n","\n","            # mlm step\n","            sents = random.choices(mdf_sentences, k=mlm_batch_size)\n","            mdf_batch = {k: v.to(model.device) for k, v in collator([tokenizer(s) for s in sents]).items()}\n","\n","            loss = loss_fn(\n","                model(**mdf_batch).prediction_logits.view(-1, model.config.vocab_size),\n","                mdf_batch['labels'].view(-1)\n","            )\n","            loss.backward()\n","            losses_mlm.append(loss.item())\n","\n","            # cross-encoder step\n","            # ce_pairs, ce_labels = get_pairs_batch(batch_size=CE_BATCH_SIZE)\n","\n","            # loss = loss_fn(\n","            #     model(\n","            #         **tokenizer(*ce_pairs, padding=True, truncation=True, max_length=128, return_tensors='pt').to(model.device)\n","            #     ).seq_relationship_logits.view(-1, 2),\n","            #     torch.tensor(ce_labels, device=model.device)\n","            # )\n","            # loss.backward()\n","            # losses_ce.append(loss.item())\n","\n","            optimizer.step()\n","            optimizer.zero_grad(set_to_none=True)\n","\n","        except RuntimeError:\n","            optimizer.zero_grad(set_to_none=True)\n","            mdf_batch, mdf_emb, ru_batch, ru_emb, all_scores, loss = None, None, None, None, None, None\n","            print('error', max(len(s) for s in mdf + ru))\n","            continue\n","        if (i + 1) % 20 == 0:\n","            print(i + 1, np.mean(losses[-20:]), np.mean(accuracies[-20:]), np.mean(losses_mlm[-20:]), np.mean(losses_ce[-20:]))\n","            if f is not None:\n","                f.write(f\"{i + 1} {np.mean(losses[-20:])} {np.mean(accuracies[-20:])} {np.mean(losses_mlm[-20:])} {np.mean(losses_ce[-20:])}\\n\")\n","\n","    return losses, accuracies, losses_mlm, losses_ce"]},{"cell_type":"markdown","id":"2816627e","metadata":{"id":"2816627e"},"source":["## train"]},{"cell_type":"code","source":["MODEL_ID = \"drive/MyDrive/diploma/labse_moksha_v3_500+3500_64bs_{}_without_CE_teacher_\" + f\"2e-5_{BATCH_SIZE}bs_{MLM_BATCH_SIZE}mlm\""],"metadata":{"id":"GfJp6XBz11yM"},"id":"GfJp6XBz11yM","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"EdCqPZ9_p--W","metadata":{"id":"EdCqPZ9_p--W"},"outputs":[],"source":["for i in range(1, 8):\n","    print(i)\n","    with open(f\"{MODEL_ID}.txt\", \"a\") as f:\n","        losses, accuracies, losses_mlm, _ = train_alignment_with_MLM_CE(\n","            parallel_pairs,\n","            mdf_sentences,\n","            100,\n","            optimizer,\n","            f\n","        )\n","\n","    print(test_model(model, tokenizer, teacher_model, teacher_tokenizer).cpu())\n","\n","    all_losses = list(itertools.chain(*[validate_model(model, tokenizer, teacher_model, teacher_tokenizer, pairs, 10) for pairs in dev_pairs.values()]))\n","    print(sum(all_losses) / len(all_losses))\n","\n","    with open(f\"{MODEL_ID}.txt\", \"a\") as f:\n","        f.write(f\"\\n{sum(all_losses) / len(all_losses)}\\n\")\n","\n","    NEW_MODEL_NAME = MODEL_ID.format(i*100)\n","    model.save_pretrained(NEW_MODEL_NAME)\n","    tokenizer.save_pretrained(NEW_MODEL_NAME)\n","    print()\n","    print()"]},{"cell_type":"code","execution_count":null,"id":"rkVtdih2FbPd","metadata":{"id":"rkVtdih2FbPd"},"outputs":[],"source":["print(test_model(model, tokenizer, teacher_model, teacher_tokenizer).cpu())\n","\n","all_losses = list(itertools.chain(*[validate_model(model, tokenizer, teacher_model, teacher_tokenizer, pairs, 10) for pairs in dev_pairs.values()]))\n","print(sum(all_losses) / len(all_losses))"]},{"cell_type":"markdown","id":"UD99Bv8Mtc5t","metadata":{"id":"UD99Bv8Mtc5t"},"source":["# Validate model on test"]},{"cell_type":"code","execution_count":null,"id":"9910b61b","metadata":{"id":"9910b61b"},"outputs":[],"source":["with open(DATA_PATH_PREFIX + 'train_test_splitting/test.json', 'r') as f:\n","    test_pairs = json.load(f)\n","print({source: len(pairs) for (source, pairs) in test_pairs.items()})"]},{"cell_type":"code","source":["MODEL_DIR = 'drive/MyDrive/diploma/labse_moksha_v0'\n","\n","\n","model = AutoModelForPreTraining.from_pretrained(MODEL_DIR)\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n","\n","model.cuda();"],"metadata":{"id":"mh5WgH6G2fRg"},"id":"mh5WgH6G2fRg","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(test_model(model, tokenizer, teacher_model, teacher_tokenizer).cpu())\n","\n","losses = {source: validate_model(model, tokenizer, teacher_model, teacher_tokenizer, pairs, 10) for (source, pairs) in test_pairs.items()}\n","\n","print({source: sum(loss) / len(loss) for (source, loss) in losses.items()})\n","\n","all_losses = list(itertools.chain(*list(losses.values())))\n","print(sum(all_losses) / len(all_losses))"],"metadata":{"id":"j08NjLtsz2VF"},"id":"j08NjLtsz2VF","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"pmEIMwmfmeLg","metadata":{"id":"pmEIMwmfmeLg"},"source":["# Active Learning"]},{"cell_type":"markdown","source":["This code was used during AL iterations"],"metadata":{"id":"5ygbviTduHx-"},"id":"5ygbviTduHx-"},{"cell_type":"code","execution_count":null,"id":"AzxlyA9wrEjs","metadata":{"id":"AzxlyA9wrEjs"},"outputs":[],"source":["# MODEL_DIR = 'drive/MyDrive/diploma/...'\n","\n","# model = AutoModelForPreTraining.from_pretrained(MODEL_DIR)\n","# tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)"]},{"cell_type":"code","execution_count":null,"id":"auvBdvVqwTnH","metadata":{"id":"auvBdvVqwTnH"},"outputs":[],"source":["batch_size = 200\n","data = parallel_pairs + list(itertools.chain(*list(test_pairs.values()))) + list(itertools.chain(*list(dev_pairs.values())))"]},{"cell_type":"code","execution_count":null,"id":"RiW1HT4CarnP","metadata":{"id":"RiW1HT4CarnP"},"outputs":[],"source":["samples = []"]},{"cell_type":"code","execution_count":null,"id":"lBIaeDreYNd8","metadata":{"id":"lBIaeDreYNd8"},"outputs":[],"source":["for i in tqdm(range(0, len(data), batch_size)):\n","    current_data = data[i:i+batch_size]\n","    current_bs = len(current_data)\n","\n","    mdf = [sample[0] for sample in current_data]\n","    ru = [sample[1] for sample in current_data]\n","\n","    with torch.inference_mode():\n","        ru_batch = tokenizer(ru, return_tensors='pt', padding=True, truncation=True, max_length=128).to(model.device)\n","        ru_embeddings = torch.nn.functional.normalize(model.bert(**ru_batch, output_hidden_states=True).pooler_output)\n","\n","        mdf_batch = tokenizer(mdf, return_tensors='pt', padding=True, truncation=True, max_length=128).to(model.device)\n","        mdf_embeddings = torch.nn.functional.normalize(model.bert(**mdf_batch, output_hidden_states=True).pooler_output)\n","\n","    for i in range(current_bs):\n","        # print(mdf[i], ru[i], (ru_embeddings[i] * mdf_embeddings[i]).sum().item())\n","        samples.append({\n","            \"mdf\": mdf[i],\n","            \"ru\": ru[i],\n","            \"score\": (ru_embeddings[i] * mdf_embeddings[i]).sum().item()\n","        })\n"]},{"cell_type":"code","execution_count":null,"id":"dxoekdwZeZ9H","metadata":{"id":"dxoekdwZeZ9H"},"outputs":[],"source":["data_df = pd.DataFrame(samples)"]},{"cell_type":"code","execution_count":null,"id":"lFvhRhV4YNgt","metadata":{"id":"lFvhRhV4YNgt"},"outputs":[],"source":["data_df.sort_values('score', ascending=True)"]},{"cell_type":"code","execution_count":null,"id":"VJDcu7Udehc0","metadata":{"id":"VJDcu7Udehc0"},"outputs":[],"source":["strange_pairs = data_df[data_df['score'] < 0.4]"]},{"cell_type":"code","execution_count":null,"id":"zH5Eqr7Bj86I","metadata":{"id":"zH5Eqr7Bj86I"},"outputs":[],"source":["strange_pairs = strange_pairs[(strange_pairs['mdf'] != '') & (strange_pairs['ru'] != '')]"]},{"cell_type":"code","execution_count":null,"id":"SQ-o3jdj0QMZ","metadata":{"id":"SQ-o3jdj0QMZ"},"outputs":[],"source":["strange_pairs.shape"]},{"cell_type":"code","execution_count":null,"id":"uz38h5KSj_pb","metadata":{"id":"uz38h5KSj_pb"},"outputs":[],"source":["strange_pairs.to_excel(DATA_PATH_PREFIX + 'small_score_pairs.xlsx')"]},{"cell_type":"code","execution_count":null,"id":"uNUJsMuhnZBW","metadata":{"id":"uNUJsMuhnZBW"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["KFydRd9gi85r","2e998e3b","b6b27915","404cfb0f","d1037df8"],"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":5}